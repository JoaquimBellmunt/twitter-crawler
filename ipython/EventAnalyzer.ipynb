{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.path.insert(0,\"../python/\")\n",
    "from search_utils import tweet_time_2_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import networkx as nx\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_coll(coll_name,mongo_port=27017,mongo_db=\"twitter-crawler\"):\n",
    "    client = MongoClient('mongodb://localhost:%i/' % mongo_port)\n",
    "    db = client[mongo_db]\n",
    "    return db[coll_name], db\n",
    "\n",
    "def find_some_docs(coll,sort_params=[('id',-1)],limit=10):\n",
    "    res = coll.find().sort(sort_params).limit(limit)\n",
    "    for item in res:\n",
    "        print(item[\"id\"],item[\"created_at\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_frequent_items(df,col,k=10):\n",
    "    val_counts = pd.DataFrame(df[col].value_counts()[:k])\n",
    "    frequent_users = [user_names[u_id] for u_id in val_counts.index]\n",
    "    res_df = pd.DataFrame()\n",
    "    res_df[\"id\"] = val_counts.index\n",
    "    res_df[\"name\"] = frequent_users\n",
    "    res_df[\"count\"] = val_counts.values\n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_graph_stats(df):\n",
    "    edges = list(zip(df[\"src\"],df[\"trg\"]))\n",
    "    G = nx.DiGraph()\n",
    "    G.add_edges_from(edges)\n",
    "    N = G.number_of_nodes()\n",
    "    M = G.number_of_edges()\n",
    "    wc_comp = nx.number_weakly_connected_components(G)\n",
    "    sc_comp = nx.number_strongly_connected_components(G)\n",
    "    return (N,M,wc_comp,sc_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_coll, db = get_coll(\"raw\")\n",
    "raw_begin_coll, _ = get_coll(\"raw_begining\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of documents in the collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_size = db.command(\"collstats\", \"raw\")[\"count\"]\n",
    "raw_begin_size = db.command(\"collstats\", \"raw_begining\")[\"count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Number of docs in 'raw': %i\" % raw_size)\n",
    "print(\"Number of docs in 'raw_begining': %i\" % raw_begin_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = raw_coll.find_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upper and lower bound of the available dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "find_some_docs(raw_begin_coll,sort_params=[(\"id\",1)],limit=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "find_some_docs(raw_begin_coll,limit=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "find_some_docs(raw_coll,sort_params=[(\"id\",1)],limit=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "find_some_docs(raw_coll,limit=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract mention networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_mentions(coll,limit=None,use_only_tweets=True):\n",
    "    res = coll.find().limit(limit) if limit != None else coll.find()\n",
    "    num_tweets, num_retweets = 0, 0\n",
    "    users = {}\n",
    "    edges = []\n",
    "    for item in res:\n",
    "        if use_only_tweets and \"RT \" == item['text'][:3]:\n",
    "            num_retweets += 1\n",
    "            continue\n",
    "        num_tweets += 1\n",
    "        src_id, epoch = item['user']['id_str'], int(tweet_time_2_epoch(item['created_at']))\n",
    "        users[src_id] = item['user']['name']\n",
    "        if 'user_mentions' in item['entities']:\n",
    "            for mention in item['entities']['user_mentions']:\n",
    "                trg_id = mention['id_str']\n",
    "                users[trg_id] = mention['name']\n",
    "                msg = item[\"text\"]\n",
    "                edges.append((epoch,src_id,trg_id,msg))\n",
    "    return edges, users, num_tweets, num_retweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a.) Extract mentions from multiple collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mentions_begin, user_names_begin, num_tweets_begin, num_retweets_begin = get_mentions(raw_begin_coll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mentions_df_begin = pd.DataFrame(mentions_begin,columns=[\"epoch\",\"src\",\"trg\",\"text\"])\n",
    "print(len(mentions_df_begin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mentions_raw, user_names_raw, num_tweets_raw, num_retweets_raw = get_mentions(raw_coll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mentions_df_raw = pd.DataFrame(mentions_raw,columns=[\"epoch\",\"src\",\"trg\",\"text\"])\n",
    "print(len(mentions_df_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b.) Rolland Garros mention network (concatenated)\n",
    "   * first part: May 23 16:51 -> May 31\n",
    "   * second part: June 01 -> WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mentions_df = pd.concat([mentions_df_begin, mentions_df_raw])\n",
    "mentions_df = mentions_df.reset_index(drop=True)\n",
    "user_names_begin.update(user_names_raw)\n",
    "user_names = user_names_begin\n",
    "num_tweets = num_tweets_begin + num_tweets_raw\n",
    "num_retweets = num_retweets_begin + num_retweets_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mentions_df[\"src_str\"] =  mentions_df[\"src\"].apply(lambda x: user_names[x])\n",
    "mentions_df[\"trg_str\"] =  mentions_df[\"trg\"].apply(lambda x: user_names[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Number of mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Number of tweets: %i\" % num_tweets)\n",
    "print(\"Number of retweets: %i\" % num_retweets)\n",
    "print(\"Number of mentions extracted from tweets: %i\" % len(mentions_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export mentions to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols_to_export = [\"epoch\",\"src\",\"trg\",\"src_str\",\"trg_str\"]\n",
    "mentions_df.to_csv(\"/mnt/idms/fberes/network/roland_garros/data/rg17_mentions.csv\",columns=cols_to_export,sep=\"|\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Popular source nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show_frequent_items(mentions_df,\"src\",k=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Popular target nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show_frequent_items(mentions_df,\"trg\",k=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Event distribution in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mentions_df[\"date\"] = mentions_df[\"epoch\"].apply(lambda x: time.strftime('%Y-%m-%d', time.localtime(x)))\n",
    "mentions_df[\"time\"] = mentions_df[\"epoch\"].apply(lambda x: time.strftime('%H:%M:%S', time.localtime(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mentions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mentions_df[\"epoch\"].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_of_mentions_by_day = mentions_df[\"date\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(num_of_mentions_by_day.values)\n",
    "plt.xticks(range(len(num_of_mentions_by_day)),num_of_mentions_by_day.index,rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Graph informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "date_splits = sorted(list(mentions_df[\"date\"].unique()))\n",
    "#date_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph_stats = [] \n",
    "for split in date_splits[1:]:\n",
    "    partial_df = mentions_df[mentions_df[\"date\"] < split]\n",
    "    graph_stats += [get_graph_stats(partial_df)]\n",
    "graph_stats_df = pd.DataFrame(graph_stats, columns=[\"nodes\",\"edges\",\"weak_components\",\"strong_components\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(16,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('Aggregated mention graph size')\n",
    "plt.plot(graph_stats_df[\"nodes\"],label=\"nodes\")\n",
    "plt.plot(graph_stats_df[\"edges\"],label=\"edges\")\n",
    "plt.xticks(range(len(graph_stats_df)),date_splits[:-1],rotation='vertical')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Number of weak components in the aggregated mention graph')\n",
    "plt.plot(graph_stats_df[\"weak_components\"],label=\"number of \",c=\"r\")\n",
    "plt.xticks(range(len(graph_stats_df)),date_splits[:-1],rotation='vertical')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dm-3-env]",
   "language": "python",
   "name": "conda-env-dm-3-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}